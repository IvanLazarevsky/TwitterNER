{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Постановка задачи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используя предоставленные данные, построить модель распознавания заданных именованных сущностей. <br>\n",
    "Разметка производится при помощи схемы B-I-O. \n",
    "\n",
    "Критерии оценки:\n",
    "Для каждого класса и в среднем - точность, полнота и f1-метрика по извлеченным сущностям. Учитываются только полные совпадения (не частичные). Будем опираться на среднюю f1 для сравнения моделей. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Результаты эксперимента.\n",
    "Результаты приведены в директории NER_experiments. Для каждой модели записана её конфигурация (meta_tags.csv), метрики в течении тренировки (metrics.csv), а также метрики на тестовой выборке (test_report.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER_common.ipynb\n",
    "Для проведения нескольких экспериментов, вынесем сюда общий код."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Средство для метрик. Интересуют точные совпадения предсказанных NE с метками и соответствующие им Precision, Recall и F1.<br>\n",
    "https://github.com/chakki-works/seqeval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seqeval in /opt/conda/lib/python3.6/site-packages (0.0.12)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.6/site-packages (3.4.5)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.6/site-packages (from seqeval) (1.16.4)\n",
      "Requirement already satisfied: Keras>=2.2.4 in /opt/conda/lib/python3.6/site-packages (from seqeval) (2.2.4)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from nltk) (1.12.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.6/site-packages (from Keras>=2.2.4->seqeval) (5.1.1)\n",
      "Requirement already satisfied: scipy>=0.14 in /opt/conda/lib/python3.6/site-packages (from Keras>=2.2.4->seqeval) (1.3.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /opt/conda/lib/python3.6/site-packages (from Keras>=2.2.4->seqeval) (1.0.8)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /opt/conda/lib/python3.6/site-packages (from Keras>=2.2.4->seqeval) (1.1.0)\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.6/site-packages (from Keras>=2.2.4->seqeval) (2.9.0)\n",
      "\u001b[33mWARNING: You are using pip version 19.2.2, however version 19.2.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install seqeval nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Упростим прототипирование при помощи pytorch-lightning. Пакет еще сырой, но автоматизирует базовые циклы обучения, чекпоинты, запись метрик."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-lightning in /opt/conda/lib/python3.6/site-packages (0.4.8)\n",
      "Requirement already satisfied: tqdm>=4.35.0 in /opt/conda/lib/python3.6/site-packages (from pytorch-lightning) (4.35.0)\n",
      "Requirement already satisfied: torch==1.2.0 in /opt/conda/lib/python3.6/site-packages (from pytorch-lightning) (1.2.0)\n",
      "Requirement already satisfied: pandas>=0.20.3 in /opt/conda/lib/python3.6/site-packages (from pytorch-lightning) (0.25.0)\n",
      "Requirement already satisfied: test-tube>=0.6.9 in /opt/conda/lib/python3.6/site-packages (from pytorch-lightning) (0.7.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from torch==1.2.0->pytorch-lightning) (1.16.4)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas>=0.20.3->pytorch-lightning) (2019.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.6/site-packages (from pandas>=0.20.3->pytorch-lightning) (2.8.0)\n",
      "Requirement already satisfied: tb-nightly==1.15.0a20190708 in /opt/conda/lib/python3.6/site-packages (from test-tube>=0.6.9->pytorch-lightning) (1.15.0a20190708)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.6/site-packages (from test-tube>=0.6.9->pytorch-lightning) (0.17.1)\n",
      "Requirement already satisfied: imageio>=2.3.0 in /opt/conda/lib/python3.6/site-packages (from test-tube>=0.6.9->pytorch-lightning) (2.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil>=2.6.1->pandas>=0.20.3->pytorch-lightning) (1.12.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.6/site-packages (from tb-nightly==1.15.0a20190708->test-tube>=0.6.9->pytorch-lightning) (3.1.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.6/site-packages (from tb-nightly==1.15.0a20190708->test-tube>=0.6.9->pytorch-lightning) (0.15.4)\n",
      "Requirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.6/site-packages (from tb-nightly==1.15.0a20190708->test-tube>=0.6.9->pytorch-lightning) (0.7.1)\n",
      "Requirement already satisfied: grpcio>=1.6.3 in /opt/conda/lib/python3.6/site-packages (from tb-nightly==1.15.0a20190708->test-tube>=0.6.9->pytorch-lightning) (1.16.1)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /opt/conda/lib/python3.6/site-packages (from tb-nightly==1.15.0a20190708->test-tube>=0.6.9->pytorch-lightning) (3.8.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.6/site-packages (from tb-nightly==1.15.0a20190708->test-tube>=0.6.9->pytorch-lightning) (41.0.1)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /opt/conda/lib/python3.6/site-packages (from tb-nightly==1.15.0a20190708->test-tube>=0.6.9->pytorch-lightning) (0.33.4)\n",
      "\u001b[33mWARNING: You are using pip version 19.2.2, however version 19.2.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pytorch-lightning flair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализация BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-transformers in /opt/conda/lib/python3.6/site-packages (1.1.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from pytorch-transformers) (4.35.0)\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.6/site-packages (from pytorch-transformers) (0.1.83)\n",
      "Requirement already satisfied: torch>=1.0.0 in /opt/conda/lib/python3.6/site-packages (from pytorch-transformers) (1.2.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from pytorch-transformers) (2.22.0)\n",
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.6/site-packages (from pytorch-transformers) (1.9.221)\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.6/site-packages (from pytorch-transformers) (2018.1.10)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from pytorch-transformers) (1.16.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->pytorch-transformers) (2019.6.16)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->pytorch-transformers) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->pytorch-transformers) (1.24.2)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->pytorch-transformers) (3.0.4)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3->pytorch-transformers) (0.9.4)\n",
      "Requirement already satisfied: botocore<1.13.0,>=1.12.221 in /opt/conda/lib/python3.6/site-packages (from boto3->pytorch-transformers) (1.12.221)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /opt/conda/lib/python3.6/site-packages (from boto3->pytorch-transformers) (0.2.1)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /opt/conda/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.221->boto3->pytorch-transformers) (0.15.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /opt/conda/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.221->boto3->pytorch-transformers) (2.8.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.13.0,>=1.12.221->boto3->pytorch-transformers) (1.12.0)\n",
      "\u001b[33mWARNING: You are using pip version 19.2.2, however version 19.2.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pytorch-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import chain, islice\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import seqeval.metrics\n",
    "from seqeval.metrics.sequence_labeling import get_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтение файла в заданном формате"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path, encoding='utf-16'):\n",
    "    inputs, targets = [],[]\n",
    "    \n",
    "    with open(path, encoding=encoding) as f:\n",
    "        current_input, current_target = [], []\n",
    "        for line in f:\n",
    "            line = line.rstrip()\n",
    "            if line:\n",
    "                token, tag = line.split()\n",
    "                current_input.append(token)\n",
    "                current_target.append(tag)\n",
    "            else:\n",
    "                inputs.append(current_input)\n",
    "                targets.append(current_target)\n",
    "                current_input, current_target = [], []\n",
    "        if current_input:\n",
    "            print(\"Doesn't end with a empty line\")\n",
    "            inputs.append(current_input)\n",
    "            targets.append(current_target)\n",
    "    return inputs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подсчёт уникальных тегов и их количества"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tags(list_of_targets):\n",
    "    return Counter(chain.from_iterable(list_of_targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Извлекаем полные named entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_spans(text, tags):\n",
    "    spans = get_entities(tags)\n",
    "    \n",
    "    return [(tag, text[i:j + 1]) for tag, i, j in spans]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_entities(list_of_targets):\n",
    "    counts = Counter()\n",
    "    for tags in list_of_targets:\n",
    "        for span in get_entities(tags):\n",
    "            counts[span[0]] += 1\n",
    "            \n",
    "    return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получение ограниченного числа образцов на каждый вариант named entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_span_tag_examples(texts, tags, examples_per_tag):\n",
    "    tag_examples = defaultdict(list)\n",
    "    filled_tags = {'O'}\n",
    "#      print(unfilled_tags)\n",
    "    for tokens, tags in zip(texts, tags):\n",
    "        for tag, span in obtain_spans(tokens, tags):\n",
    "            if tag not in filled_tags:\n",
    "                tag_examples[tag].append(span)\n",
    "                if len(tag_examples[tag]) >= examples_per_tag:\n",
    "                    filled_tags.add(tag)\n",
    "    return tag_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Класс словаря, в конечном итоге использовался только для тегов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, word_counts, size, specials, unk_index):\n",
    "        self._word_list = []\n",
    "        self._word_list.extend(specials)\n",
    "        for w,_ in word_counts.most_common(size-len(specials)):\n",
    "            self._word_list.append(w)\n",
    "        self._reverse_index = {w:i for i,w in enumerate(self._word_list)}\n",
    "        self.unk_index = unk_index\n",
    "        self.n_specials = len(specials)\n",
    "        \n",
    "    @classmethod\n",
    "    def from_id2word(cls,id2word, unk_index, n_specials):\n",
    "        self = cls.__new__(cls)\n",
    "        self._word_list = [x for x in id2word]\n",
    "        self._reverse_index = {w:i for i,w in enumerate(self._word_list)}\n",
    "        self.unk_index = unk_index\n",
    "        self.n_specials = n_specials\n",
    "        return self\n",
    "    \n",
    "    @staticmethod\n",
    "    def build_on_tokens(tokenized_text, specials, max_size, unk_index):\n",
    "        word_counts = Counter(chain.from_iterable(tokenized_text))\n",
    "        return Vocab(word_counts, max_size, specials, unk_index)\n",
    "        \n",
    "    def word2id(self, w):\n",
    "        idx = self._reverse_index.get(w)\n",
    "        if idx is not None:\n",
    "            return idx\n",
    "        else:\n",
    "            return self.unk_index\n",
    "        \n",
    "    def id2word(self, idx):\n",
    "        return self._word_list[idx]\n",
    "    \n",
    "    def transform_tokens(self, text, drop_unk=False):\n",
    "        result = []\n",
    "        for tok in text:\n",
    "            idx = self.word2id(tok)\n",
    "            if idx == self.unk_index and drop_unk:\n",
    "                continue\n",
    "            result.append(idx)\n",
    "        return result\n",
    "    \n",
    "    def numericalize(self, texts, drop_unk=False):\n",
    "        result = []\n",
    "        for text in texts:\n",
    "            result.append(self.transform_tokens(text, drop_unk))\n",
    "        return result\n",
    "    \n",
    "    def transform_ids(self, ids):\n",
    "        return [self.id2word(x) for x in ids]\n",
    "    \n",
    "    def denumericalize(self, encoded):\n",
    "        return [self.transform_ids(ids) for ids in encoded]\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self._word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Базовая предобработка - замена URL на спец. слово, расщепление хэштегов, никнеймов (@nickname) и замена чисел на спец.слово.\n",
    "Маска используется, чтобы разрешать вопрос о теге расщепленного слова. В этом случае будет использоваться тег первой части токена.\n",
    "<b>TODO: Использовать более продвинутый парсер URL</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_tag(tag, times):\n",
    "    if tag.startswith('B-'):\n",
    "        return [tag] + ['I-' + tag[2:]] * (times - 1)\n",
    "    else:\n",
    "        return [tag] * times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_preprocessing(inputs, targets, replace_urls=True, split_hashtags=False, split_mentions=False, replace_numbers=False):\n",
    "    new_inputs, new_targets = [], []\n",
    "    target_masks = []\n",
    "    for tokens, labels in zip(inputs, targets):\n",
    "        new_tokens = []\n",
    "        new_labels = []\n",
    "        mask = []\n",
    "        for i in range(len(tokens)):\n",
    "            token, tag = tokens[i], labels[i]\n",
    "            if replace_urls:\n",
    "                if token.startswith(\"https://\") or token.startswith(\"http://\"):\n",
    "                    new_tokens.append('<URL>')\n",
    "                    new_labels.append(tag)\n",
    "                    mask.append(True)\n",
    "                    continue\n",
    "            if split_hashtags and token.startswith('#') and len(token) > 1:\n",
    "                new_tokens.append('#')\n",
    "                new_tokens.append(token[1:])\n",
    "                new_labels.extend(split_tag(tag, 2))\n",
    "                mask.extend((True, False))\n",
    "                continue\n",
    "            if split_mentions and token.startswith('@') and len(token) > 1:\n",
    "                new_tokens.append('@')\n",
    "                new_tokens.append(token[1:])\n",
    "                new_labels.extend(split_tag(tag, 2))\n",
    "                mask.extend((True, False))\n",
    "                continue\n",
    "            if replace_numbers and token.isnumeric():\n",
    "                new_tokens.append('<NUM>')\n",
    "                new_labels.append(tag)\n",
    "                mask.append(True)\n",
    "                continue\n",
    "            new_tokens.append(token)\n",
    "            new_labels.append(tag)\n",
    "            mask.append(True)\n",
    "        new_inputs.append(new_tokens)\n",
    "        new_targets.append(new_labels)\n",
    "        target_masks.append(mask)\n",
    "        \n",
    "    return new_inputs, new_targets, target_masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сплит для сравнения результатов. Чтобы не повторять его в отдельных блокнотах, назначим выборку каждому индексу.<br> Обучающая выборка используется для обучения модели, валидационная - для настройки гиперпараметров, ранней остановки, и отслеживания прогресса.<br> Контрольная выборка используется для окончательного теста, в процессе разработки нужно избежать её использования для обратной связи (поэтому запись результатов в файл производится вслепую)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_seed = 19450\n",
    "np.random.seed(split_seed)\n",
    "training_set_size = 5200\n",
    "validation_set_size = 843\n",
    "test_set_size = 1200\n",
    "\n",
    "train_val_indices, test_indices = train_test_split(np.arange(7243), test_size=test_set_size)\n",
    "train_indices, val_indices = train_test_split(train_val_indices, test_size=validation_set_size)\n",
    "assert len(train_indices) == training_set_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разбиение массивов, используя выборки индексов. \n",
    "Пример split_by_indices([texts, targets], [train, val]) == [texts_train, texts_val, targets_train, targets_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_indices(arrays, splits):\n",
    "    result = []\n",
    "    for array in arrays:\n",
    "        array = np.asarray(array)\n",
    "        for split in splits:\n",
    "            split = np.asarray(split)\n",
    "            result.append(array[split])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разбиение iterable на группы по n элементов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grouper(n, iterable):\n",
    "    it = iter(iterable)\n",
    "    while True:\n",
    "        chunk = tuple(islice(it, n))\n",
    "        if not chunk:\n",
    "            return\n",
    "        yield chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку отслеживать множество переменных вида texts_train, texts_val, targets_test становится затруднительно, для каждого массива создадим словарь с ключами для соотв. выборок.  <br>\n",
    "Пример выхода: [{'train': texts_train, 'val': texts_val }, {'train': targets_train, 'val': targets_val }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_to_dicts(arrays, splits, split_names):\n",
    "    assert len(splits) == len(split_names)\n",
    "    all_splits = split_by_indices(arrays, splits)\n",
    "    result = []\n",
    "    for chunk in grouper(len(splits), all_splits):\n",
    "        d = dict(zip(split_names, chunk))\n",
    "        result.append(d)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, не импортирован ли блокнот. Если нет, проверим код и проведём небольшой анализ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "directly_executed =  __name__ == '__main__' and '__file__' not in globals()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример предобработки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RT', '@KianLawley', ':', '@flowerfulkian', 'TOMORROW', 'IS', 'MY', 'LAST', 'DAY', '!!']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['RT', '@', 'KianLawley', ':', '@', 'flowerfulkian', 'TOMORROW', 'IS', 'MY', 'LAST', 'DAY', '!!']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "[True, True, False, True, True, False, True, True, True, True, True, True]\n"
     ]
    }
   ],
   "source": [
    "if directly_executed:\n",
    "    inputs, targets = read_data('data/data.txt')\n",
    "    example_id = 5\n",
    "    print(inputs[example_id])\n",
    "    print(targets[example_id])\n",
    "    preproc_inputs, preproc_targets, preproc_masks = basic_preprocessing(inputs, targets, replace_urls=True, split_hashtags=True, split_mentions=True)\n",
    "    print(preproc_inputs[example_id])\n",
    "    print(preproc_targets[example_id])\n",
    "    print(preproc_masks[example_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример разбиения по индексам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: \n",
      "train 5200\n",
      "['Jan', '19', 'My', 'daily', 'investment', 'tips', 'today', 'for', 'Empire', 'Avenue', '#EAv', '!', 'http://t.co/E14XfhsIIM', 'via', '@wordpressdotcom']\n",
      "val 843\n",
      "['RT', '@jeromegodefroy', ':', 'Les', 'catholiques', ',', 'ces', 'fondamentalistes', 'assassins', ':', 'https://t.co/TQ80d9pmHQ']\n",
      "test 1200\n",
      "['@YourboyH', 'cool', \"I'll\", 'check', 'it', 'out', 'when', 'I', 'get', 'home', '.']\n",
      "Targets: \n",
      "train 5200\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-other', 'I-other', 'O', 'O', 'O', 'O', 'O']\n",
      "val 843\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "test 1200\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "if directly_executed:\n",
    "    input_split, target_split = split_to_dicts([inputs, targets],\n",
    "                                               [train_indices, val_indices, test_indices],\n",
    "                                               ['train', 'val', 'test'])\n",
    "    \n",
    "    preproc_input_split, preproc_target_split = split_to_dicts([preproc_inputs, preproc_targets], \n",
    "                                                               [train_indices, val_indices, test_indices],\n",
    "                                                               ['train', 'val', 'test'])\n",
    "    print('Inputs: ')\n",
    "    for k,v in input_split.items():\n",
    "        print(k, len(v))\n",
    "        print(v[0])\n",
    "        \n",
    "    print('Targets: ')\n",
    "    for k,v in target_split.items():\n",
    "        print(k, len(v))\n",
    "        print(v[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверка, насколько сократилось число уникальных слов в обучающей выборке после препроцессинга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in training set (no preprocessing) 22500\n",
      "Total words in training set (no urls, hashtag split, nickname split) 19756\n"
     ]
    }
   ],
   "source": [
    "if directly_executed:\n",
    "    unique_words = set(chain.from_iterable(input_split['train']))\n",
    "    print('Total words in training set (no preprocessing)', len(unique_words))\n",
    "    unique_words_preproc = set(chain.from_iterable(preproc_input_split['train']))\n",
    "    print('Total words in training set (no urls, hashtag split, nickname split)', len(unique_words_preproc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим распределение тегов в выборках (используем classification_report для красивой печати)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "     company          608 0.15\n",
      "    facility          277 0.07\n",
      "         loc          892 0.22\n",
      "       movie           54 0.01\n",
      " musicartist          206 0.05\n",
      "       other          664 0.17\n",
      "      person          781 0.19\n",
      "     product          287 0.07\n",
      "  sportsteam          193 0.05\n",
      "      tvshow           44 0.01\n",
      "---------\n",
      "val\n",
      "     company           84 0.13\n",
      "    facility           58 0.09\n",
      "         loc          156 0.24\n",
      "       movie           17 0.03\n",
      " musicartist           33 0.05\n",
      "       other          110 0.17\n",
      "      person          131 0.20\n",
      "     product           38 0.06\n",
      "  sportsteam           21 0.03\n",
      "      tvshow           11 0.02\n",
      "---------\n",
      "test\n",
      "     company          139 0.14\n",
      "    facility           60 0.06\n",
      "         loc          226 0.23\n",
      "       movie           12 0.01\n",
      " musicartist           48 0.05\n",
      "       other          167 0.17\n",
      "      person          190 0.20\n",
      "     product           55 0.06\n",
      "  sportsteam           54 0.06\n",
      "      tvshow           14 0.01\n",
      "---------\n"
     ]
    }
   ],
   "source": [
    "if directly_executed:\n",
    "    row_format =\"{:>12} {:>12} {:.2f}\"\n",
    "    for split, targets in target_split.items():\n",
    "        print(split)\n",
    "        tag_counts = count_entities(targets)\n",
    "        total_tags = sum(tag_counts.values())\n",
    "        for tag, count in sorted(tag_counts.items(), key=lambda t: t[0]):\n",
    "            print(row_format.format(tag, count, count / total_tags))\n",
    "        print('---------')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим пропорцию неизвестных слов в тегах. Для этого половину слов из обучающей выборке поместим в словарь. Проверка пропорций осуществляется на валидационной выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "known_words/total_words (training set) 0.8738804380723918\n",
      "known_words/total_words (validation set) 0.7763408946364214\n",
      "known_words_in_tags/total_words_in_tags (validation set) 0.5623318385650224\n",
      "loc 120 206 0.5825242718446602\n",
      "company 88 116 0.7586206896551724\n",
      "product 65 126 0.5158730158730159\n",
      "person 85 199 0.4271356783919598\n",
      "other 109 189 0.5767195767195767\n",
      "facility 90 136 0.6617647058823529\n",
      "musicartist 28 53 0.5283018867924528\n",
      "movie 21 39 0.5384615384615384\n",
      "tvshow 10 20 0.5\n",
      "sportsteam 11 31 0.3548387096774194\n"
     ]
    }
   ],
   "source": [
    "if directly_executed:\n",
    "    word_counts = Counter(chain.from_iterable(input_split['train']))\n",
    "    tag_counts = Counter(chain.from_iterable(target_split['train']))\n",
    "    unk_index = 0\n",
    "    vocab = Vocab(word_counts, size=len(unique_words) // 2, specials=['<UNK>'], unk_index=unk_index)\n",
    "    total_words = 0\n",
    "    known_words = 0\n",
    "    for w in chain.from_iterable(input_split['train']):\n",
    "        total_words += 1\n",
    "        if vocab.word2id(w) != unk_index:\n",
    "            known_words += 1\n",
    "    \n",
    "    print('known_words/total_words (training set)', known_words/total_words)\n",
    "    \n",
    "    val_total_words = sum(1 for _ in chain.from_iterable(input_split['val']))\n",
    "    val_known_words = sum(1 for w in chain.from_iterable(input_split['val']) if w in vocab._reverse_index)\n",
    "    \n",
    "    print('known_words/total_words (validation set)', val_known_words/val_total_words)\n",
    "    \n",
    "    known_words_in_tags = defaultdict(lambda: 0)\n",
    "    total_words_in_tags = defaultdict(lambda: 0)\n",
    "    for text, tags in zip(input_split['val'], target_split['val']):\n",
    "        for tag, span in obtain_spans(text, tags):\n",
    "            for word in span:\n",
    "                if vocab.word2id(word) != unk_index:\n",
    "                    known_words_in_tags[tag] += 1\n",
    "                total_words_in_tags[tag] += 1\n",
    "                \n",
    "    \n",
    "    \n",
    "    print('known_words_in_tags/total_words_in_tags (validation set)', \n",
    "          sum(known_words_in_tags.values()) / sum(total_words_in_tags.values()))\n",
    "    \n",
    "    for tag in total_words_in_tags.keys():\n",
    "        k, t = known_words_in_tags[tag], total_words_in_tags[tag]\n",
    "        print(tag, k, t, k/t if t != 0 else 'inf')\n",
    "        \n",
    "    \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем примеры named entity каждого типа."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "other : Empire Avenue|Super Bowl|Christmas|How To Get A Paying Job In The Film Industry|#MemorialDay|GPD|New Year|the Great Exhibition|Quds|#UN\n",
      "\n",
      "product : Conflicts of Law : Cases and Materials|Sour Patch kids|Lounge22|12 \" 3PCS Set Disney Frozen Queenx Elsa Princesses Anna Olaf Snowman Dolls Toys|Lincoln park after dark|Russian navy|Vintage Banner space helmet &amp; goggles|Exo browser|#Chrome|soulful house mix\n",
      "\n",
      "person : Lea Brilmayer|Jack L|Andrew Michaelis|Tim|ansley|God|nini|Jon T . Tumilson Chief Special Warfare Operator|Obama|Anja Rubik\n",
      "\n",
      "loc : Minneapolis|Clemson|Auburn|#Budapest|Afghanistan|Australia|U . S .|Mesa|Manchester|Ill\n",
      "\n",
      "facility : #Keleti|Murray State|OMNIA Nightclub|#Wax Club Bangkok|The Pub|Bedford Farmers Market|Washington Navy Yard|The White House|ROSS TOWNSHIP BUILDING|the oasis\n",
      "\n",
      "company : G+|Snapchat|BBC News|Playboy|Playboy|NBC|CORT|TalkTalk|Kmart Australia|Kmart Australia\n",
      "\n",
      "sportsteam : The Wildcats|Bolton|Liverpool|Reds|Colts|Philadelphia Eagles|PERAK|TERENGGANU|Eyeopener|Ryerson Quidditch\n",
      "\n",
      "tvshow : Saturday Night Live|Eat Bulaga Kalye Serye|Today Show|Private Practice|Miley and Mandy show|Rurouni Kenshin|#BattlestarGalactica|#StarWars #TheCloneWars|PangakoSayo|#TheFlash\n",
      "\n",
      "musicartist : Justin Bieber|DJ Chris L|U2|Gucci Mane|dj manolo|Luke|Justin|KISS|Future|the Stone Roses\n",
      "\n",
      "movie : Piranha 3D|JENNIFERS BODY|A New Hope|escape from new york|rocky horror show|Mockingjay : Part 1|Princess Lover OVA 1|#DaysofFuturePast|The Room|#StarWars\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if directly_executed:\n",
    "    tag_examples = obtain_span_tag_examples(input_split['train'], target_split['train'], 10)\n",
    "    for tag, examples in tag_examples.items():\n",
    "        print(tag,\":\",'|'.join([' '.join(ex) for ex in examples]))\n",
    "        print()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выводы:<br>\n",
    "1) Имена собственные содержат большую долю редких слов, чем окружающий текст.<br>\n",
    "2) Регистр букв является значимым признаком. По этой причине его стоит либо сохранить, либо отделить (например, к эмбеддингу слова прибавлять эмбеддинг регистра)<br>\n",
    "3) Признаки на уровне символов или частей слов также имеют роль. Например, спортивные команды часто заканчиваются на 's'. Хештеги также могут использоваться с named entity.<br>\n",
    "4) Рассмотреть вариант использования spell-checker <br>\n",
    "5) Данных не много, поэтому имеет смысл в первую очередь использовать претренированные модели или представления."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модели, которые планируется протестировать:<br>\n",
    "Pre-trained embeddings + BiLSTM <br>\n",
    "Pre-trained embeddings + BiLSTM + CRF <br>\n",
    "BERT fine-tuning (готово) <br>\n",
    "Pre-trained embeddings примеры: Flair, Fasttext, BERT, ELMo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
